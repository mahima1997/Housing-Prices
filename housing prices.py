# -*- coding: utf-8 -*-
"""
Created on Tue May 30 23:04:20 2017

@author: hp
"""
#USED LINEAR REGRESSION AND THEN LASSO LINEAR REGRESSION
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('ggplot')
plt.rcParams['figure.figsize'] = (10, 6)
from sklearn.model_selection import train_test_split
from sklearn import linear_model
from sklearn.metrics import mean_squared_error
#from sklearn.metrics import accuracy_score  ...accuracy score but R-squared is calculated for linear regression 


test_data=pd.read_csv('E:\\1MAHIMA GUPTA\\DOCS\\COLLEGE\\ML\\random assignments\\Housing Prices Test.csv',encoding = "iso-8859-15")
train_data=pd.read_csv('E:\\1MAHIMA GUPTA\\DOCS\\COLLEGE\\ML\\random assignments\\Housing prices train.csv',encoding = "iso-8859-15")
#train_data.isnull().any()
#train_data = train_data.fillna(method='ffill')
print(train_data)
print("shape of train_data=",train_data.shape)
print("header of train_data=\n",train_data.head(5))
print("Analysis of sales price:\n",train_data.SalePrice.describe())

#Lets check the skewness of our train_data
print ("Skew is:", train_data.SalePrice.skew())
plt.hist(train_data.SalePrice, color='blue')
plt.show()                  # X-axis is salesPrice while Y axis is frequency

#When performing regression, sometimes it makes sense to log-transform the target variable when it is skewed.
#One reason for this is to improve the linearity of the train_data. Although the justification is beyond the scope of 
#this tutorial, more information can be found here. Importantly, the predictions generated by the final model will 
#also be log-transformed, so we’ll need to convert these predictions back to their original form later.

####np.log() will transform the variable, and np.exp() will reverse the transformation.####

target = np.log(train_data.SalePrice)
print ("Skew is:", target.skew())
plt.hist(target, color='blue')
plt.show()
#hence skewness has been dealt with..value is close to 0 i.e normal distribution.

#Knowing the numeric predictors
numeric_features = train_data.select_dtypes(include=[np.number])
print("Numeric predictors:\n",numeric_features.dtypes)

#Knowing the relaton between predictors and response variables
corr = numeric_features.corr()
print ("\nMost correlated features: \n",corr['SalePrice'].sort_values(ascending=False)[:10], '\n')
print ("\nLesser correlated factors:\n",corr['SalePrice'].sort_values(ascending=False)[-10:])

#The OverallQual(predictor that is rating the overall material and finish of the house) in train_data are
#integer values in the interval 1 to 10 inclusive.
#So let's take OverallQual to be the index.
#The aggregation functions are applied to the values you list.
#We can create a pivot table to further investigate the relationship between OverallQual and SalePrice.
#This table is telling the median sale price against the ratings for Overall Quality of the house
train_data.OverallQual.unique()
quality_pivot = train_data.pivot_table(index='OverallQual',values='SalePrice', aggfunc=np.median)
print(quality_pivot)

quality_pivot.plot(kind='bar', color='blue')
plt.xlabel('Overall Quality')
plt.ylabel('Median Sale Price')
plt.xticks(rotation=0)
plt.show()

#let’s use plt.scatter() to generate some scatter plots and visualize the relationship between 
#the Ground living area of House and SalePrice.
plt.scatter(x=train_data['GrLivArea'], y=target)
plt.ylabel('Sale Price')
plt.xlabel('Above grade (ground) living area square feet')
plt.show()

#let’s use plt.scatter() to generate some scatter plots and visualize the relationship between 
#the Garage area of House and SalePrice.
plt.scatter(x=train_data['GarageArea'], y=target)
plt.ylabel('Sale Price')
plt.xlabel('Garage Area')
plt.show()
#Notice that there are many homes with 0 for Garage Area, indicating that they don’t have a garage. We’ll 
#transform other features later to reflect this assumption There are a few outliers as well. Outliers can affect 
#a regression model by pulling our estimated regression line further away from the true population regression line.
#So, we’ll remove those observations from our train_data. 
#Removing outliers is an art and a science. There are many techniques for dealing with outliers.

train_data = train_data[train_data['GarageArea'] < 1200]
plt.scatter(x=train_data['GarageArea'], y=np.log(train_data.SalePrice))     #can't use y=target here bcoz x and y 
plt.xlim(-200,1600) # This forces the same scale as before                  #should be of same size      
plt.ylabel('Sale Price')
plt.xlabel('Garage Area')
plt.show()

#We will create a DataFrame to view the top null columns. Chaining together 
#the train.isnull().sum() methods, we return a Series of the counts of the null values in each column.
#### here 'NA' and not '0' values in the dataset are being considered as null bcoz the fact that garage area 
#### is '0' is being taken care in the fact that garageQual/type/finish/yrBlt are all 'NA'.
nulls = pd.DataFrame(train_data.isnull().sum().sort_values(ascending=False)[:25])
nulls.columns = ['Null Count']
nulls.index.name = 'Feature'
print(nulls)

#In MiscFeature column, we’ll use the Series.unique() method to return a list of the unique values.
print ("Unique values of MiscFeature predictor are:", train_data.MiscFeature.unique())

#Now lets deal with the non numeric features
categoricals = train_data.select_dtypes(exclude=[np.number])
print('\n',categoricals.describe())         #top is the most frequently occuring value

##Our model expects that the shape of the features from the train set match those from the test set. This 
##means that any feature engineering that occurred while working on the train data should be applied again 
##on the test set.
print ("Original: \n") 
print (train_data.Street.value_counts(), "\n")

#Now we encode this column 'street' to enc_street both in train and test data into numbers
train_data['enc_street'] = pd.get_dummies(train_data.Street, drop_first=True)    #by default,drop_first is false
test_data['enc_street'] = pd.get_dummies(test_data.Street, drop_first=True)
print ('Encoded: \n') 
print (train_data.enc_street.value_counts())
print (train_data.Street.value_counts())
#Let’s try engineering another feature. We’ll look at SaleCondition by constructing and plotting a pivot table,
#as we did above for OverallQual.

condition_pivot = train_data.pivot_table(index='SaleCondition', values='SalePrice', aggfunc=np.median)
condition_pivot.plot(kind='bar', color='blue')
print("\n original:\n",condition_pivot)
plt.xlabel('Sale Condition')
plt.ylabel('Median Sale Price')
plt.xticks(rotation=0)
plt.show()

#Notice that Partial has a significantly higher Median Sale Price than the others. We will encode this as a new
#feature. We select all of the houses where SaleCondition is equal to Patrial and assign the value 1, otherwise 
#assign 0.
def encode(x): return 1 if x == 'Partial' else 0
train_data['enc_condition'] = train_data.SaleCondition.apply(encode) #pd.get_dummies not used so we had 
test_data['enc_condition'] = test_data.SaleCondition.apply(encode)   #to specify encode function o/w it
print (train_data.SaleCondition.value_counts())                      #it would have encoded by itself
#print(train_data)

condition_pivot = train_data.pivot_table(index='enc_condition', values='SalePrice', aggfunc=np.median)
print("\n encoded:\n",condition_pivot)
condition_pivot.plot(kind='bar', color='blue')
plt.xlabel('Encoded Sale Condition')
plt.ylabel('Median Sale Price')
plt.xticks(rotation=0)
plt.show()

#The dropna can used to drop rows or columns with missing data (NaN). By default, it drops all rows with any
#missing entry.
print("No. of columns with null value: ",sum(train_data.isnull().sum() != 0)) 
data = train_data.select_dtypes(include=[np.number]).interpolate().dropna()   
print("\nAfter dropping No. of columns with null value: ",sum(data.isnull().sum() != 0))         
#----RESOLVE----which of the columns are dropped

#We’ll separate the features and the target variable for modeling. 
#We will assign the features to X and the target variable to y.
y = np.log(train_data.SalePrice)
X = data.drop(['SalePrice', 'Id'], axis=1)  #id is dropped bcoz it is not a predictor and does not affect the response variable

#now we divide the train_data into train and test to check applicability of model
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33)

#Lets first create a linear regression model
lr = linear_model.LinearRegression()
model = lr.fit(X_train, y_train)

#we want to evaluate the performance of the model through R-squared
print ("R^2 is: ", model.score(X_test, y_test))

#now we will evaluate the performance of the model through RMSE
predictions = model.predict(X_test)
print ('RMSE is: ', mean_squared_error(y_test, predictions))

#The RMSE measures the distance between our predicted values and actual values.
#We can view this relationship graphically with a scatter plot.
actual_values = y_test
plt.scatter(predictions, actual_values, alpha=.50,color='b')        #alpha helps to show overlapping data and a  
plt.xlabel('Predicted Price')                                       #rough measure of density. Larger values
plt.ylabel('Actual Price')                                          #of alpha specify stronger regularization
plt.title('Linear Regression Model')
plt.show()

#We’ll next try to improve our model using Ridge Regularization to decrease the influence of less imp features. 
#Ridge Regularization is a process which shrinks the regression coefficients of less important features.
for i in range (-2, 3):
    alpha = 10**i
    rm = linear_model.Ridge(alpha=alpha)
    ridge_model = rm.fit(X_train, y_train)
    preds_ridge = ridge_model.predict(X_test)
    
    plt.scatter(preds_ridge, actual_values, alpha=.5, color='b')
    plt.xlabel('Predicted Price')
    plt.ylabel('Actual Price')
    plt.title('Ridge Regularization with alpha = {}'.format(alpha))
    overlay = 'R^2 is: {}\nRMSE is: {}'.format(ridge_model.score(X_test, y_test),mean_squared_error(y_test, preds_ridge))
    plt.annotate(s=overlay,xy=(12.1,10.6),size='x-large')
    plt.show()
#These models perform almost identically to the first model. In our case, adjusting 
#the alpha did not substantially improve our model. As we add more features, regularization can be helpful.     

submission = pd.DataFrame()
submission['Id'] = test_data.Id
feats = test_data.select_dtypes(include=[np.number]).drop(['Id'], axis=1).interpolate()
#feats=test_data.drop(['Id'], axis=1)----------------RESOLVE THE ERROR-----------
predictions = model.predict(feats)

#Remember that to reverse log() we do exp(). So we will apply np.exp() to our predictions becasuse we 
#have taken the logarithm previously.
final_predictions = np.exp(predictions)
print ("Original predictions are: \n", predictions[:5], "\n")
print ("Final predictions are: \n", final_predictions[:5])
submission['SalePrice'] = final_predictions
print(submission.head())

#We pass index=False because Pandas otherwise would create a new index for us.
submission.to_csv('Housing Prices submission.csv', index=False)











